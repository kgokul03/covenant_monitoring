{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "88f0f4ce-68b2-4c4b-abd5-5738d60aeae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kawin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import torch\n",
    "import ollama\n",
    "from openai import OpenAI\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0c851ee1-c4a3-4fd1-96b7-34bfb051dc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 450\n",
    "CYAN = '\\033[96m'\n",
    "NEON_GREEN = '\\033[92m'\n",
    "RESET_COLOR = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e5bf0c4-f6be-403c-89fb-39fa8e3ca794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFromPdf(pdf_path):\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_path)\n",
    "    num_pages = len(pdf_reader.pages)\n",
    "    text = ''\n",
    "    for page_num in range(num_pages):\n",
    "        page = pdf_reader.pages[page_num]\n",
    "        if page.extract_text():\n",
    "            text += page.extract_text() + \" \"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4eb8ff32-f3a6-415b-b238-944a22d4d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFromPdf(pdf_path):\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_path)\n",
    "    text_list = []\n",
    "    \n",
    "    for page in pdf_reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        \n",
    "        if page_text:\n",
    "            # Remove common headers and footers (adjust regex if needed)\n",
    "            lines = page_text.split(\"\\n\")\n",
    "            \n",
    "            # Regex pattern to detect headers like: \"1/8/25, 10:05 AM sec.gov/Archives/...\"\n",
    "            header_pattern = r'\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APM]{2}'                         \n",
    "\n",
    "            # Other common headers/footers to remove\n",
    "            unwanted_patterns = [\n",
    "                header_pattern,                      # Header with date & sec.gov URL\n",
    "                r'EX-\\d+\\.\\d+',\n",
    "                r\"^Page\\s*\\d+\",                      # Matches \"Page X\"\n",
    "                r'sec\\.gov/Archives/\\S+',\n",
    "                r\"^(Document Title|Confidential)\",   # Common header/footer texts\n",
    "                r\"^SEC\\s*Filing\\s*Details\",          # SEC-specific header\n",
    "                r\"^Table of Contents$\",              # Table of Contents header\n",
    "                r\"^(http)\"                           # remove the link/url\n",
    "            ]\n",
    "\n",
    "            # Remove lines matching any of the patterns\n",
    "            filtered_lines = [line for line in lines if not any(re.match(p, line.strip()) for p in unwanted_patterns)]\n",
    "            \n",
    "            # Replace newlines with spaces for cleaner text output\n",
    "            clean_text = \" \".join(filtered_lines)\n",
    "            text_list.append(clean_text)\n",
    "    \n",
    "    return \" \".join(text_list)  # Join all pages with a space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca954973-e993-4b59-a1ce-b493bceed22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareChunks(text):\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Split text into chunks by sentences, respecting a maximum chunk size\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)  # split on spaces following sentence-ending punctuation\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        # Check if the current sentence plus the current chunk exceeds the limit\n",
    "        if len(current_chunk) + len(sentence) + 1 < CHUNK_SIZE:  # +1 for the space\n",
    "            current_chunk += (sentence + \" \").strip()\n",
    "        else:\n",
    "            # When the chunk exceeds 1000 characters, store it and start a new one\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sentence + \" \"\n",
    "    if current_chunk:  # Don't forget the last chunk!\n",
    "        chunks.append(current_chunk)\n",
    "    with open(\"vault.txt\", \"w\", encoding=\"utf-8\") as vault_file:\n",
    "        for chunk in chunks:\n",
    "            # Write each chunk to its own line\n",
    "            vault_file.write(chunk.strip() + \"\\n\\n\")  # Two newlines to separate chunks\n",
    "    print(f\"PDF content stored to vault.txt with each chunk on a separate line.\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "98c05950-532c-421c-b3cb-face08a45244",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Efficient semantic embeddings\n",
    "\n",
    "def prepareChunks(text, chunk_size=1000, output_file=\"vault.txt\", similarity_threshold=0.7):\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Use spaCy for sentence segmentation\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_chunk_embedding = None  # Store embeddings for semantic similarity checks\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_embedding = embedder.encode(sentence, convert_to_tensor=True)\n",
    "\n",
    "        # If adding a sentence exceeds chunk size OR it's semantically different → Start a new chunk\n",
    "        if (sum(len(s) for s in current_chunk) + len(sentence) + 1 > chunk_size or\n",
    "                (current_chunk_embedding is not None and \n",
    "                 util.pytorch_cos_sim(current_chunk_embedding, sentence_embedding).item() < similarity_threshold)):\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_chunk_embedding = sentence_embedding\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            # Update chunk embedding as average of existing embeddings\n",
    "            current_chunk_embedding = sentence_embedding if current_chunk_embedding is None else \\\n",
    "                                      (current_chunk_embedding + sentence_embedding) / 2\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    # Save chunks to a file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as vault_file:\n",
    "        vault_file.write(\"\\n\\n\".join(chunks))  # Efficient file writing\n",
    "\n",
    "    print(f\"PDF content stored in {output_file} with semantic chunking.\")\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0ba19a11-13eb-4577-b1dd-f9b36e27066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareEmbeddings(chunks):\n",
    "    print(NEON_GREEN + \"Generating embeddings for the vault content...\" + RESET_COLOR)\n",
    "    vault_embeddings = []\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            response = ollama.embeddings(model='nomic-embed-text', prompt=chunk)\n",
    "            embedding = response.get(\"embedding\")\n",
    "            if embedding:  # Ensure embedding is not None\n",
    "                vault_embeddings.append(embedding)\n",
    "            else:\n",
    "                print(f\"Skipping invalid embedding for content: {chunk.strip()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate embedding for content: {chunk.strip()}. Error: {e}\")\n",
    "\n",
    "    if not vault_embeddings:\n",
    "        print(\"No valid embeddings generated. Exiting...\")\n",
    "        exit(1)\n",
    "    return vault_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d8c492d6-451b-43a6-bfd5-47e9ea4b6e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertEmbeddingsToTensor(vault_embeddings):\n",
    "    # Ensure all embeddings have the same size\n",
    "    embedding_size = len(vault_embeddings[0])\n",
    "    if any(len(e) != embedding_size for e in vault_embeddings):\n",
    "        print(\"Embedding size mismatch detected. Skipping invalid embeddings...\")\n",
    "        vault_embeddings = [e for e in vault_embeddings if len(e) == embedding_size]\n",
    "\n",
    "    # Convert to tensor\n",
    "    vault_embeddings_tensor = torch.tensor(vault_embeddings)\n",
    "    print(\"Embeddings for each line in the vault:\")\n",
    "    print(vault_embeddings_tensor)\n",
    "    return vault_embeddings_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "eacdadc7-0dc0-415b-98ae-2f91788a56cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_context(rewritten_input, vault_embeddings, vault_content, top_k=5):\n",
    "    if not rewritten_input.strip():\n",
    "        print(\"Rewritten input is empty. Skipping context retrieval.\")\n",
    "        return []\n",
    "\n",
    "    if vault_embeddings.nelement() == 0:  # Check if the tensor has any elements\n",
    "        print(\"Vault embeddings are empty. Skipping context retrieval.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        input_embedding = ollama.embeddings(model='nomic-embed-text', prompt=rewritten_input)[\"embedding\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate input embedding. Error: {e}\")\n",
    "        return []\n",
    "\n",
    "    if not input_embedding:\n",
    "        print(\"Input embedding is invalid. Skipping context retrieval.\")\n",
    "        return []\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cos_scores = torch.cosine_similarity(torch.tensor(input_embedding).unsqueeze(0), vault_embeddings)\n",
    "\n",
    "    # Adjust top_k if needed\n",
    "    top_k = min(top_k, len(cos_scores))\n",
    "    top_indices = torch.topk(cos_scores, k=top_k)[1].tolist()\n",
    "\n",
    "    # Retrieve relevant context\n",
    "    relevant_context = [vault_content[idx].strip() for idx in top_indices]\n",
    "    return relevant_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cabe6795-5e54-4165-9b8d-b75c6f8bfcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelevantEmbeddings(query, embeddings, content):\n",
    "    relevant_context = get_relevant_context(query, embeddings, content)\n",
    "    if relevant_context:\n",
    "        context_str = \"\\n\".join(relevant_context)\n",
    "        print(\"Context Pulled from Documents: \\n\\n\" + CYAN + context_str + RESET_COLOR)\n",
    "    else:\n",
    "        print(CYAN + \"No relevant context found.\" + RESET_COLOR)\n",
    "    \n",
    "    user_input_with_context = query\n",
    "    if relevant_context:\n",
    "        user_input_with_context = query + \"\\n\\nRelevant Context:\\n\" + context_str\n",
    "\n",
    "    conversation_history = []\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input_with_context})\n",
    "\n",
    "\n",
    "    system_message = \"You are a helpful assistant that is an expert at extracting the most useful information from a given text. Also bring in extra relevant information to the user query from outside the given context.\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message}, \n",
    "        *conversation_history\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model='gemma:2b',\n",
    "        messages=messages,\n",
    "        max_tokens=3000,\n",
    "    )\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "23c7164b-7065-41e1-b8a6-d15a194eb2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1',\n",
    "    api_key='gemma:2b'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d20fe3f7-cbe7-4a67-8fdb-e64dded4f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = getDataFromPdf(\"./Example_2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4526edcf-c222-4787-a7e9-5f4986f753a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF content stored in vault.txt with semantic chunking.\n"
     ]
    }
   ],
   "source": [
    "chunks = prepareChunks(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1fb53aa6-499e-4fd9-b190-ebbdae3f5532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mGenerating embeddings for the vault content...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "embeddings = prepareEmbeddings(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b6fc6eff-57e5-4a35-be7b-e9f810241370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings for each line in the vault:\n",
      "tensor([[ 0.8608,  0.9354, -3.0653,  ..., -1.1735, -0.7083, -0.1666],\n",
      "        [ 0.6117,  0.0962, -3.4848,  ..., -0.6500, -1.4436, -0.8836],\n",
      "        [ 0.8844,  1.1359, -3.4204,  ..., -1.3753, -0.8387, -0.8021],\n",
      "        ...,\n",
      "        [ 1.1135,  1.4939, -3.5688,  ..., -1.6604, -0.9151, -0.4049],\n",
      "        [ 0.7071,  1.2066, -3.8221,  ..., -1.3988, -0.9617,  0.1832],\n",
      "        [ 1.2377,  0.7432, -3.4734,  ..., -1.9130, -1.0240,  0.1240]])\n"
     ]
    }
   ],
   "source": [
    "tensor_embedding = convertEmbeddingsToTensor(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1b8f4857-828e-435b-a226-83c45a1ea916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(ques):\n",
    "    getRelevantEmbeddings(ques, tensor_embedding, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6970d4a3-6b9b-4af5-867c-2c52f480e872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Pulled from Documents: \n",
      "\n",
      "\u001b[96mThe Borrower and its Subsidiaries taken as a whole are Solvent as of the Effective Date.\n",
      "The Administrative Agent shall have received, at least three Business Days prior to the Effective Date, all documentation and other information regarding the Borrower requested in connection with applicable “know your customer” and anti-money laundering rules and regulations, including the Patriot Act, to the extent requested in writing of the Borrower at least 10 Business Days prior to the Effective Date and (ii) to the extent the Borrower qualifies as a “legal entity customer” under the Beneficial Ownership Regulation, at least three Business Days prior to the Effective Date, any Lender that has requested, in a written notice to the Borrower at least 10 Business Days prior to the Effective Date, a Beneficial Ownership Certification in relation to the Borrower shall have received such Beneficial Ownership Certification (provided that, upon the execution and delivery by such Lender of its signature page to this Agreement, the condition set forth in this clause (f) shall be deemed to be satisfied).\n",
      "(a) The Borrower shall have the right at any time and from time to time to prepay any Borrowing in whole or in part, subject to prior notice in accordance with the provisions of this Section 2.11(a). The Borrower shall notify the Administrative Agent by written notice of any prepayment hereunder (i) in the case of prepayment of a Term Benchmark Borrowing, not later than 11:00 a.m., New York City time, three Business Days before the date of prepayment, (ii) in the case of prepayment of an RFR Borrowing, not later than 11:00 a.m., New York City time, five (5) Business Days before the date of prepayment or (iii) in the case of prepayment of an ABR Borrowing, not later than 11:00 a.m., New York City time, two (2) Business Days before the date of prepayment.\n",
      "For purposes of determining whether the conditions specified in this Section 4.01 have been satisfied on the Effective Date, by funding the Loans hereunder on the Effective Date, the Administrative Agent and each Lender, as applicable, shall be deemed to have consented to, approved or accepted, or to be satisfied with, each document or other matter required hereunder to be consented to or approved by or acceptable or satisfactory to the Administrative Agent or such Lender, as the case may be. The Administrative Agent shall notify the Borrower and the Lenders of the Effective Date, and such notice shall be conclusive and binding.\n",
      "For purposes hereof, the date of a Borrowing initially shall be the date on which such Borrowing is made and thereafter shall be the effective date of the most recent conversion or continuation of such Borrowing.\u001b[0m\n",
      "**Borrower: <To be specified by the Borrower in the Loan Agreement**\n",
      "\n",
      "**Administrative Agent:**\n",
      "* Name: <To be specified by the Administrative Agent in the Loan Agreement>\n",
      "* Contact: <To be specified by the Administrative Agent in the Loan Agreement>\n",
      "\n",
      "**Underwriter:**\n",
      "* Name: <To be specified by the Underwriter in the Loan Agreement>\n",
      "* Contact: <To be specified by the Underwriter in the Loan Agreement>\n",
      "\n",
      "**Effective Date:** \n",
      "* This context does not mention the date of the Borrower and its subsidiaries taking effect, so I cannot extract the information from the context.\n"
     ]
    }
   ],
   "source": [
    "ques='extract the Borrower, admistrative agent and underwriter(Bookrunner, lead arranger, left lead, manager) and the aggrement date'\n",
    "get(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f49a8b43-fd3b-464e-9d10-e525c4cda30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exhibit 10.1 EXECUTION VERSION CREDIT AGREEMENT dated as of July 26, 2023 among HARMONY BIOSCIENCES HOLDINGS, INC., as Borrower The Lenders Party Hereto JPMORGAN CHASE BANK, N.A. as Administrative Agent and JPMORGAN CHASE BANK, N.A., as Bookrunner and Lead Arranger iTABLE OF CONTENTS Page Article I Definitions 1 Section 1.01Defined\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove document references (e.g., \"EX-10.1 2 hrmy-20230726xex10d1.htm EX-10.1\")\n",
    "    text = re.sub(r'EX-\\d+\\.\\d+ \\d+ \\S+ EX-\\d+\\.\\d+', '', text)\n",
    "\n",
    "    # Remove timestamps (e.g., \"1/8/25, 10:05 AM\")\n",
    "    text = re.sub(r'\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APM]{2}', '', text)\n",
    "\n",
    "    # Remove SEC URLs\n",
    "    text = re.sub(r'sec\\.gov/Archives/\\S+', '', text)\n",
    "\n",
    "    # Normalize whitespace (remove extra spaces and newlines)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "raw_text = \"\"\"EX-10.1 2 hrmy-20230726xex10d1.htm EX-10.1 Exhibit 10.1 EXECUTION VERSION CREDIT AGREEMENT dated as of July 26, 2023 among HARMONY BIOSCIENCES HOLDINGS, INC., as Borrower The Lenders Party Hereto JPMORGAN CHASE BANK, N.A. as Administrative Agent and JPMORGAN CHASE BANK, N.A., as Bookrunner and Lead Arranger1/8/25, 10:05 AM sec.gov/Archives/edgar/data/1802665/000155837023012348/hrmy-20230726xex10d1.htm#_Toc256000083 iTABLE OF CONTENTS Page Article I Definitions 1 Section 1.01Defined\"\"\"\n",
    "\n",
    "cleaned_text = clean_text(raw_text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d1aa1f-58f4-45ec-82a6-0780ad77f543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
